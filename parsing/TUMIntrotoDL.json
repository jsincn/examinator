{
  "total_points": 0,
  "total_time_min": 90,
  "exam_content": {
    "problems": [
      {
        "total_points": 18,
        "sub_questions": [
          {
            "question_text_latex": "a) Which of the following statements regarding successful ImageNet-classification architectures are correct?",
            "question_options": [
              "ResNet18 has 11 million parameters more than VGG16.",
              "AlexNet uses filters of different kernel sizes.",
              "InceptionV3 uses filters of different kernel sizes.",
              "VGG16 only uses convolutional layers."
            ],
            "question_correct_option_indices": [
              1,
              2
            ],
            "question_points": 2.0,
            "calculation_function": "= 2 TAN(PI(0.0188888888888889)+PI(0.0655555555555556))^2",
            "show_mc_notes": false,
            "show_corrections": false,
            "option_corrections": null
          },
          {
            "question_text_latex": "b) You train a neural network and the train loss diverges. What are reasonable things to do? (check all that apply)",
            "question_options": [
              "Decrease the learning rate.",
              "Add dropout.",
              "Increase the learning rate.",
              "Try a different optimizer."
            ],
            "question_correct_option_indices": [
              0,
              3
            ],
            "question_points": 2.0,
            "calculation_function": "= 2 TAN(PI(0.0188888888888889)+PI(0.0655555555555556))^2",
            "show_mc_notes": false,
            "show_corrections": false,
            "option_corrections": null
          },
          {
            "question_text_latex": "c) What is the correct order of operations for an optimization with gradient descent?",
            "question_options": [
              "bcdea",
              "ebadc",
              "eadbc",
              "edbac"
            ],
            "question_correct_option_indices": [
              3
            ],
            "question_points": 2.0,
            "calculation_function": "= 2 TAN(PI(0.0188888888888889)+PI(0.0655555555555556))^2",
            "show_mc_notes": false,
            "show_corrections": false,
            "option_corrections": null
          },
          {
            "question_text_latex": "d) Consider a simple convolutional neural network with a single convolutional layer. Which of the following statements is true about this network?",
            "question_options": [
              "It is rotation invariant.",
              "It is translation equivariant.",
              "All input nodes are connected to all output nodes.",
              "It is scale-invariant."
            ],
            "question_correct_option_indices": [
              1
            ],
            "question_points": 2.0,
            "calculation_function": "= 2 TAN(PI(0.0188888888888889)+PI(0.0655555555555556))^2",
            "show_mc_notes": false,
            "show_corrections": false,
            "option_corrections": null
          },
          {
            "question_text_latex": "e) Which of the following activation functions can lead to vanishing gradients?",
            "question_options": [
              "Tanh.",
              "ReLU.",
              "Sigmoid.",
              "Leaky Relu."
            ],
            "question_correct_option_indices": [
              0,
              2
            ],
            "question_points": 2.0,
            "calculation_function": "= 2 TAN(PI(0.0188888888888889)+PI(0.0655555555555556))^2",
            "show_mc_notes": false,
            "show_corrections": false,
            "option_corrections": null
          },
          {
            "question_text_latex": "f) Logistic regression (check all that apply).",
            "question_options": [
              "Is a linear function.",
              "Is a supervised learning algorithm.",
              "Uses a type of cross-entropy loss.",
              "Allows to perform binary classification."
            ],
            "question_correct_option_indices": [
              1,
              2,
              3
            ],
            "question_points": 2.0,
            "calculation_function": "= 2 TAN(PI(0.0188888888888889)+PI(0.0655555555555556))^2",
            "show_mc_notes": false,
            "show_corrections": false,
            "option_corrections": null
          },
          {
            "question_text_latex": "g) A sigmoid layer",
            "question_options": [
              "cannot be used during backpropagation.",
              "has a learnable parameter.",
              "maps surjectively to values in (-1, 1), i.e., hits all values in that interval.",
              "is continuous and differentiable everywhere."
            ],
            "question_correct_option_indices": [
              3
            ],
            "question_points": 2.0,
            "calculation_function": "= 2 TAN(PI(0.0188888888888889)+PI(0.0655555555555556))^2",
            "show_mc_notes": false,
            "show_corrections": false,
            "option_corrections": null
          },
          {
            "question_text_latex": "h) Your training loss does not decrease. What could be wrong?",
            "question_options": [
              "Learning rate is too high.",
              "Too much regularization.",
              "Dropout probability not high enough.",
              "Bad initialization."
            ],
            "question_correct_option_indices": [
              0,
              1,
              3
            ],
            "question_points": 2.0,
            "calculation_function": "= 2 TAN(PI(0.0188888888888889)+PI(0.0655555555555556))^2",
            "show_mc_notes": false,
            "show_corrections": false,
            "option_corrections": null
          },
          {
            "question_text_latex": "i) Which of the following have trainable parameters? (check all that apply)",
            "question_options": [
              "Leaky ReLU",
              "Batch normalization",
              "Dropout",
              "Max pooling"
            ],
            "question_correct_option_indices": [
              1
            ],
            "question_points": 2.0,
            "calculation_function": "= 2 TAN(PI(0.0188888888888889)+PI(0.0655555555555556))^2",
            "show_mc_notes": false,
            "show_corrections": false,
            "option_corrections": null
          }
        ],
        "question_title": "Problem 1",
        "show_instructions": false,
        "option_corrections": null
      },
      {
        "total_points": 8,
        "sub_questions": [
          {
            "question_text_latex": "a) Compute a forward pass for each set of weights and draw the results of the linear layer in the Figure of the tanh plot. You don’t need to compute the tanh.",
            "question_answer_latex": "Var(w) = 0.02:  s = (2 * 0.05) + (−4 * 0.025) + (1 * (−0.03)) = −0.03 \\ (0.5p) \nVar(w) = 1.0: s = (2 * 1.0) + (−4 * 0.5) + (1 * 1.5)) = 1.5 \\ (0.5p) \n0.5p for each of the calculated numbers drawn correctly into the plot. \\ \\text{Note: Inaccurate drawing is tolerated, but positive instead of negative isn’t (e.g. if you drew your s (=x) in the negative axis side but the correct one is in the positive axis)}",
            "available_points": 2.0,
            "starred": true,
            "box_height": "3"
          }
        ],
        "question_title": "Problem 2",
        "question_description_latex": "Activation Functions and Weight Initialization (8 credits)"
      },
      {
        "total_points": 6,
        "sub_questions": [
          {
            "question_text_latex": "a) Why would one want to apply batch normalization in a neural network?",
            "question_answer_latex": "To reduce Internal (not plain covariate shift) covariate shift - i.e. to normalize input features for each layer to get stable gradients during training. (1p) \\ \\text{Note: Only 0.5p for stable gradient, regularization, able to train deeper network, stabilize training, faster training ...}",
            "available_points": 1.0,
            "starred": false,
            "box_height": "3"
          },
          {
            "question_text_latex": "b) Why are \\( \\gamma \\) and \\( \\beta \\) needed in the batch normalization formula?",
            "question_answer_latex": "Allows the network to \"undo\" or scale/shift the distribution after the normalization. (1p) \\ \\text{Note: 0.5p if only one of scaling and shift are mentioned\\ Stating that gamma and beta are hyperparameters does not get any points}",
            "available_points": 1.0,
            "starred": false,
            "box_height": "3"
          },
          {
            "question_text_latex": "c) How is a batch normalization layer applied at training (1p) and at test (1p) time?",
            "question_answer_latex": "Train time: \\text{Calculate the mean and variance for each feature across the mini batch} (0.5p) \\text{and \\ accumulate a moving average across training mini-batches for the update at test time} (0.5p) \nTest time: \\text{Use exponentially weighted / moving / running average mean and variance that was computed at training time on the test samples (1p)} \\ \\text{Note: 0.5p only if just simply mentioned acquiring statistics from the training set, but failed to illustrate which method is needed to acquire mean and var, i.e. exponentially weighted average}",
            "available_points": 2.0,
            "starred": false,
            "box_height": "3"
          }
        ],
        "question_title": "Problem 3",
        "question_description_latex": "Batch Normalization and Computation Graphs (6 credits)"
      },
      {
        "total_points": 12,
        "sub_questions": [
          {
            "question_text_latex": "a) You are given a two dimensional input (e.g., a grayscale image). Consider the following convolutional kernels\n\n\\[ C_1 = \\frac{1}{9} \\cdot \\begin{bmatrix}1 & 1 & 1 \\\\1 & 1 & 1 \\\\1 & 1 & 1 \\end{bmatrix} \\ , \\ C_2 = \\begin{bmatrix}1 \\ & −1\\ \\ & −1\\ \\end{bmatrix} \\]",
            "question_answer_latex": "\\begin{align*}\nC_1: \\ & \\text{Local/box (0.5p) blur/smoothing (0.5p) kernel. Note: If only mean/arg is mentioned instead of blur then 0.5p} \\\\\nC_2: \\ & \\text{vertical (0.5p) edge detector (0.5p)} \\\\\n\\end{align*}",
            "available_points": 2.0,
            "starred": false,
            "box_height": "3"
          }
        ],
        "question_title": "Problem 4",
        "question_description_latex": "Convolutional Neural Networks and Receptive Field (12 credits)"
      },
      {
        "total_points": 15,
        "sub_questions": [
          {
            "question_text_latex": "a) How would you split your dataset and give a meaningful percentage as answer.",
            "question_answer_latex": "0.5 pt: Train + Validation + Test (must mention all 3)\n0.5 pt: meaningful percentages (training split must be > 50%)",
            "available_points": 1.0,
            "starred": false,
            "box_height": "3"
          }
        ],
        "question_title": "Problem 5",
        "question_description_latex": "Training a Neural Network (15 credits)"
      },
      {
        "total_points": 9,
        "sub_questions": [
          {
            "question_text_latex": "a) Given the dimensions \\( x_t \\in \\mathbb{R}^3 \\) and \\( h_t \\in \\mathbb{R}^5 \\), what is the number of parameters in the RNN cell? (Calculate final number)",
            "question_answer_latex": "3 \\times 5 + 5 \\times 5 + 5(\\text{bias}) = 15 + 25 + 5 = 45 \\ \\text{(1p for 45, else 0)}",
            "available_points": 1.0,
            "starred": false,
            "box_height": "3"
          }
        ],
        "question_title": "Problem 6",
        "question_description_latex": "Recurrent Neural Networks and Backpropagation (9 credits)"
      },
      {
        "total_points": 11,
        "sub_questions": [
          {
            "question_text_latex": "a) Is an autoencoder an example of unsupervised learning or supervised learning?",
            "question_answer_latex": "\\text{unsupervised learning (1p) if you give the wrong reasoning after correct choice then only 0.5p}",
            "available_points": 1.0,
            "starred": false,
            "box_height": "3"
          }
        ],
        "question_title": "Problem 7",
        "question_description_latex": "Autoencoder and Network Transfer (11 credits)"
      },
      {
        "total_points": 11,
        "sub_questions": [
          {
            "question_text_latex": "a) Why do we need activation functions in our neural networks?",
            "question_answer_latex": "\\text{Introduce non-linearity (1p), other wise the network can be reduced to a single linear layer. (alternatively: to model non-linear functions).}",
            "available_points": 1.0,
            "starred": false,
            "box_height": "3"
          }
        ],
        "question_title": "Problem 8",
        "question_description_latex": "Unsorted Short Questions (11 credits)"
      }
    ]
  },
  "exam_title": "Introduction to Deep Learning",
  "examiner": "Prof. Leal-Taixé and Prof. Nießner",
  "module": "IN2346 / Endterm",
  "start_time": "08:00",
  "end_time": "09:30",
  "exam_chair": ""
}